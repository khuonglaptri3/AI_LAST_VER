import torch
import numpy as np
import matplotlib.pyplot as plt
from collections import defaultdict
import time
from environment import SnakeEnv
from model import Linear_QNet

# =========================
# Config
# =========================
STATE_SIZE = 16
HIDDEN_SIZE = 256
ACTION_SIZE = 3  # Model ƒë√£ train v·ªõi 3 actions (straight/right/left)

class Tester:
    def __init__(self, model_path, phase=3):
        """
        model_path: ƒë∆∞·ªùng d·∫´n t·ªõi file .pth
        phase: 1 (10x10), 2 (15x15), 3 (20x20)
        """
        self.phase = phase
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        # Load model
        self.model = Linear_QNet(STATE_SIZE, HIDDEN_SIZE, ACTION_SIZE)
        self.model.load_state_dict(torch.load(model_path, map_location=self.device))
        self.model.to(self.device)
        self.model.eval()  # evaluation mode
        
        print(f"‚úÖ Model loaded from: {model_path}")
        print(f"üì± Device: {self.device}")
    
    def get_action(self, state, current_direction, greedy=True):
        """
        L·∫•y action t·ª´ model v√† convert sang absolute direction
        Model output: [straight, right, left]
        """
        state_tensor = torch.tensor(state, dtype=torch.float, device=self.device)
        with torch.no_grad():
            q_values = self.model(state_tensor)
        
        if greedy:
            relative_action = torch.argmax(q_values).item()
        else:
            probs = torch.softmax(q_values, dim=0)
            relative_action = torch.multinomial(probs, 1).item()
        
        # Convert relative action to absolute direction
        return self._relative_to_absolute(relative_action, current_direction)
    
    def _relative_to_absolute(self, relative_action, current_dir):
        """
        Convert relative action (0=straight, 1=right, 2=left) 
        to absolute direction (0=up, 1=down, 2=left, 3=right)
        """
        clock_wise = [0, 3, 1, 2]  # up, right, down, left
        idx = clock_wise.index(current_dir)
        
        if relative_action == 0:  # straight
            return clock_wise[idx]
        elif relative_action == 1:  # turn right
            return clock_wise[(idx + 1) % 4]
        else:  # turn left
            return clock_wise[(idx - 1) % 4]
    
    def create_env(self, render_mode="human"):
        """T·∫°o environment theo phase"""
        if self.phase == 1:
            w, h = 10, 10
        elif self.phase == 2:
            w, h = 15, 15
        elif self.phase == 3:
            w, h = 20, 20
        else:
            raise ValueError("Phase ph·∫£i l√† 1, 2 ho·∫∑c 3")
        
        options = {
            "fps": 10,  # t·ªëc ƒë·ªô hi·ªÉn th·ªã
            "max_step": 1000,
            "init_length": 3,
            "food_reward": 10.0,
            "dist_reward": 1.0,
            "living_bonus": -0.01,
            "death_penalty": -50.0,
            "width": w,
            "height": h,
            "block_size": 20,
        }
        
        return SnakeEnv(render_mode=render_mode, **options)
    
    # =========================
    # TEST 1: Ch∆°i visual v·ªõi pygame
    # =========================
    def play_visual(self, num_games=5, fps=10, greedy=True):
        """
        Xem agent ch∆°i tr·ª±c ti·∫øp v·ªõi pygame
        num_games: s·ªë game mu·ªën ch∆°i
        fps: t·ªëc ƒë·ªô hi·ªÉn th·ªã
        greedy: True = ch·ªçn action t·ªët nh·∫•t, False = stochastic
        """
        env = self.create_env(render_mode="human")
        env.snake.fps = fps
        
        for game in range(1, num_games + 1):
            obs, _ = env.reset()
            done, truncated = False, False
            total_reward = 0
            steps = 0
            
            print(f"\n{'='*50}")
            print(f"üéÆ Game {game}/{num_games}")
            print(f"{'='*50}")
            
            while not (done or truncated):
                state = np.array(obs, dtype=float)
                current_dir = env.snake.direction
                action = self.get_action(state, current_dir, greedy=greedy)
                
                obs, reward, done, truncated, info = env.step(action)
                total_reward += reward
                steps += 1
                
                # Hi·ªÉn th·ªã
                env.render()
                time.sleep(1.0 / fps)
            
            print(f" K·∫øt qu·∫£:")
            print(f"   Score: {env.snake.score}")
            print(f"   Steps: {steps}")
            print(f"   Total Reward: {total_reward:.2f}")
            print(f"   Reason: {' Dead' if done else ' Timeout'}")
            
            if game < num_games:
                input("\n  Press Enter ƒë·ªÉ ch∆°i game ti·∫øp theo...")
        
        env.close()
    
    # =========================
    # TEST 2: ƒê√°nh gi√° performance (kh√¥ng hi·ªÉn th·ªã)
    # =========================
    def evaluate(self, num_episodes=100, greedy=True):
        """
        ƒê√°nh gi√° performance tr√™n nhi·ªÅu episodes
        Tr·∫£ v·ªÅ statistics
        """
        env = self.create_env(render_mode=None)
        
        scores = []
        steps_list = []
        rewards_list = []
        reasons = {"dead": 0, "timeout": 0}
        
        print(f"\nüî¨ ƒêang ƒë√°nh gi√° model tr√™n {num_episodes} episodes...")
        
        for ep in range(1, num_episodes + 1):
            obs, _ = env.reset()
            done, truncated = False, False
            total_reward = 0
            steps = 0
            
            while not (done or truncated):
                state = np.array(obs, dtype=float)
                current_dir = env.snake.direction
                action = self.get_action(state, current_dir, greedy=greedy)
                obs, reward, done, truncated, _ = env.step(action)
                
                total_reward += reward
                steps += 1
            
            scores.append(env.snake.score)
            steps_list.append(steps)
            rewards_list.append(total_reward)
            
            if done:
                reasons["dead"] += 1
            else:
                reasons["timeout"] += 1
            
            if ep % 10 == 0:
                print(f"Progress: {ep}/{num_episodes} episodes")
        
        env.close()
        
        # T√≠nh statistics
        stats = {
            "mean_score": np.mean(scores),
            "std_score": np.std(scores),
            "max_score": np.max(scores),
            "min_score": np.min(scores),
            "mean_steps": np.mean(steps_list),
            "mean_reward": np.mean(rewards_list),
            "success_rate": (reasons["timeout"] / num_episodes) * 100,
            "death_rate": (reasons["dead"] / num_episodes) * 100,
        }
        
        # In k·∫øt qu·∫£
        print(f"\n{'='*60}")
        print(f" K·∫æT QU·∫¢ ƒê√ÅNH GI√Å ({num_episodes} episodes)")
        print(f"{'='*60}")
        print(f"Score Statistics:")
        print(f"   Mean ¬± Std: {stats['mean_score']:.2f} ¬± {stats['std_score']:.2f}")
        print(f"   Max: {stats['max_score']}")
        print(f"   Min: {stats['min_score']}")
        print(f"\n  Steps/Rewards:")
        print(f"   Mean Steps: {stats['mean_steps']:.1f}")
        print(f"   Mean Reward: {stats['mean_reward']:.2f}")
        print(f"\n Death Analysis:")
        print(f"   Survival Rate: {stats['success_rate']:.1f}%")
        print(f"   Death Rate: {stats['death_rate']:.1f}%")
        print(f"{'='*60}\n")
        
        # V·∫Ω bi·ªÉu ƒë·ªì
        self._plot_evaluation(scores, steps_list, rewards_list)
        
        return stats, scores, steps_list, rewards_list
    
    def _plot_evaluation(self, scores, steps, rewards):
        """V·∫Ω bi·ªÉu ƒë·ªì k·∫øt qu·∫£ evaluation"""
        fig, axes = plt.subplots(2, 2, figsize=(14, 10))
        fig.suptitle(f'Model Evaluation Results - Phase {self.phase}', fontsize=16)
        
        # Score histogram
        axes[0, 0].hist(scores, bins=20, color='skyblue', edgecolor='black', alpha=0.7)
        axes[0, 0].axvline(np.mean(scores), color='red', linestyle='--', 
                          label=f'Mean: {np.mean(scores):.2f}')
        axes[0, 0].set_xlabel('Score')
        axes[0, 0].set_ylabel('Frequency')
        axes[0, 0].set_title('Score Distribution')
        axes[0, 0].legend()
        axes[0, 0].grid(True, alpha=0.3)
        
        # Score over episodes
        axes[0, 1].plot(scores, alpha=0.6, linewidth=1)
        axes[0, 1].axhline(np.mean(scores), color='red', linestyle='--', 
                          label=f'Mean: {np.mean(scores):.2f}')
        axes[0, 1].set_xlabel('Episode')
        axes[0, 1].set_ylabel('Score')
        axes[0, 1].set_title('Score over Episodes')
        axes[0, 1].legend()
        axes[0, 1].grid(True, alpha=0.3)
        
        # Steps histogram
        axes[1, 0].hist(steps, bins=20, color='lightgreen', edgecolor='black', alpha=0.7)
        axes[1, 0].axvline(np.mean(steps), color='red', linestyle='--',
                          label=f'Mean: {np.mean(steps):.1f}')
        axes[1, 0].set_xlabel('Steps')
        axes[1, 0].set_ylabel('Frequency')
        axes[1, 0].set_title('Steps Distribution')
        axes[1, 0].legend()
        axes[1, 0].grid(True, alpha=0.3)
        
        # Reward histogram
        axes[1, 1].hist(rewards, bins=20, color='salmon', edgecolor='black', alpha=0.7)
        axes[1, 1].axvline(np.mean(rewards), color='red', linestyle='--',
                          label=f'Mean: {np.mean(rewards):.2f}')
        axes[1, 1].set_xlabel('Total Reward')
        axes[1, 1].set_ylabel('Frequency')
        axes[1, 1].set_title('Reward Distribution')
        axes[1, 1].legend()
        axes[1, 1].grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(f'evaluation_phase{self.phase}.png', dpi=150, bbox_inches='tight')
        print(f" Bi·ªÉu ƒë·ªì ƒë√£ l∆∞u: evaluation_phase{self.phase}.png")
        plt.show()
    
    # =========================
    # TEST 3: So s√°nh Greedy vs Stochastic
    # =========================
    def compare_strategies(self, num_episodes=50):
        """So s√°nh hi·ªáu su·∫•t gi·ªØa greedy v√† stochastic policy"""
        print("\n So s√°nh Greedy vs Stochastic Policy...")
        
        print("\n1Ô∏è Testing Greedy Policy...")
        stats_greedy, scores_g, _, _ = self.evaluate(num_episodes, greedy=True)
        
        print("\n2Ô∏è Testing Stochastic Policy...")
        stats_stoch, scores_s, _, _ = self.evaluate(num_episodes, greedy=False)
        
        # So s√°nh
        print(f"\n{'='*60}")
        print(" COMPARISON RESULTS")
        print(f"{'='*60}")
        print(f"{'Metric':<20} {'Greedy':<15} {'Stochastic':<15} {'Winner'}")
        print(f"{'-'*60}")
        
        metrics = [
            ("Mean Score", stats_greedy['mean_score'], stats_stoch['mean_score']),
            ("Max Score", stats_greedy['max_score'], stats_stoch['max_score']),
            ("Mean Steps", stats_greedy['mean_steps'], stats_stoch['mean_steps']),
            ("Success Rate %", stats_greedy['success_rate'], stats_stoch['success_rate']),
        ]
        
        for name, g_val, s_val in metrics:
            winner = "Greedy" if g_val > s_val else "Stochastic" if s_val > g_val else "Tie"
            print(f"{name:<20} {g_val:<15.2f} {s_val:<15.2f} {winner}")
        
        print(f"{'='*60}\n")
        
        # V·∫Ω so s√°nh
        fig, axes = plt.subplots(1, 2, figsize=(14, 5))
        
        axes[0].hist(scores_g, bins=15, alpha=0.6, label='Greedy', color='blue')
        axes[0].hist(scores_s, bins=15, alpha=0.6, label='Stochastic', color='orange')
        axes[0].set_xlabel('Score')
        axes[0].set_ylabel('Frequency')
        axes[0].set_title('Score Distribution Comparison')
        axes[0].legend()
        axes[0].grid(True, alpha=0.3)
        
        axes[1].plot(scores_g, alpha=0.7, label='Greedy', linewidth=2)
        axes[1].plot(scores_s, alpha=0.7, label='Stochastic', linewidth=2)
        axes[1].set_xlabel('Episode')
        axes[1].set_ylabel('Score')
        axes[1].set_title('Score over Episodes')
        axes[1].legend()
        axes[1].grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig(f'comparison_phase{self.phase}.png', dpi=150)
        print(f"üíæ So s√°nh ƒë√£ l∆∞u: comparison_phase{self.phase}.png")
        plt.show()
    
    # =========================
    # TEST 4: Ph√¢n t√≠ch h√†nh vi
    # =========================
    def analyze_behavior(self, num_episodes=20):
        """Ph√¢n t√≠ch h√†nh vi c·ªßa agent"""
        env = self.create_env(render_mode=None)
        
        action_counts = defaultdict(int)
        collision_types = defaultdict(int)
        
        print(f"\nüî¨ Ph√¢n t√≠ch h√†nh vi tr√™n {num_episodes} episodes...")
        
        for ep in range(num_episodes):
            obs, _ = env.reset()
            done, truncated = False, False
            
            while not (done or truncated):
                state = np.array(obs, dtype=float)
                current_dir = env.snake.direction
                action = self.get_action(state, current_dir, greedy=True)
                
                action_counts[action] += 1
                obs, reward, done, truncated, _ = env.step(action)
            
            # Ph√¢n lo·∫°i nguy√™n nh√¢n ch·∫øt
            if done:
                head = env.snake.head
                if head.x < 0 or head.x >= env.snake.blocks_x or \
                   head.y < 0 or head.y >= env.snake.blocks_y:
                    collision_types["wall"] += 1
                else:
                    collision_types["self"] += 1
            else:
                collision_types["timeout"] += 1
        
        env.close()
        
        # In k·∫øt qu·∫£
        print(f"\n{'='*60}")
        print("üéØ BEHAVIOR ANALYSIS")
        print(f"{'='*60}")
        
        total_actions = sum(action_counts.values())
        print("\nüìç Action Distribution:")
        action_names = {0: "UP", 1: "DOWN", 2: "LEFT", 3: "RIGHT"}
        for action in range(4):
            count = action_counts.get(action, 0)
            pct = (count / total_actions * 100) if total_actions > 0 else 0
            print(f"   {action_names[action]:<8}: {count:>6} ({pct:>5.1f}%)")
        
        print("\nüíÄ Death Causes:")
        total_deaths = sum(collision_types.values())
        for cause, count in collision_types.items():
            pct = (count / total_deaths * 100) if total_deaths > 0 else 0
            print(f"   {cause.capitalize():<10}: {count:>4} ({pct:>5.1f}%)")
        
        print(f"{'='*60}\n")
        
        # V·∫Ω bi·ªÉu ƒë·ªì
        fig, axes = plt.subplots(1, 2, figsize=(12, 5))
        
        # Action distribution
        actions = [action_names[i] for i in range(4)]
        counts = [action_counts.get(i, 0) for i in range(4)]
        axes[0].bar(actions, counts, color=['blue', 'green', 'orange', 'red'], alpha=0.7)
        axes[0].set_ylabel('Count')
        axes[0].set_title('Action Distribution')
        axes[0].grid(True, alpha=0.3, axis='y')
        
        # Death causes
        causes = list(collision_types.keys())
        cause_counts = list(collision_types.values())
        axes[1].pie(cause_counts, labels=causes, autopct='%1.1f%%', startangle=90)
        axes[1].set_title('Death Causes')
        
        plt.tight_layout()
        plt.savefig(f'behavior_phase{self.phase}.png', dpi=150)
        print(f" Ph√¢n t√≠ch ƒë√£ l∆∞u: behavior_phase{self.phase}.png")
        plt.show()


# =========================
# MAIN - V√≠ d·ª• s·ª≠ d·ª•ng
# =========================
if __name__ == "__main__":
    # ƒê∆∞·ªùng d·∫´n t·ªõi model ƒë√£ train
    MODEL_PATH = r"D:\AL_FINAL_PROJECT_LAST_WEEK\AI_LAST_VER\model\dqn_snake_phase3_best.pth"
    PHASE = 3
    
    # Kh·ªüi t·∫°o tester
    tester = Tester(MODEL_PATH, phase=PHASE)
    
    # Ch·ªçn ch·∫ø ƒë·ªô test:
    print("\n" + "="*60)
    print(" SNAKE AI MODEL TESTING")
    print("="*60)
    print("Ch·ªçn ch·∫ø ƒë·ªô test:")
    print("1.  Xem agent ch∆°i (visual v·ªõi pygame)")
    print("2.  ƒê√°nh gi√° performance (100 episodes)")
    print("3.   So s√°nh Greedy vs Stochastic")
    print("4.  Ph√¢n t√≠ch h√†nh vi")
    print("5.  Ch·∫°y t·∫•t c·∫£ tests")
    print("="*60)
    
    choice = input("\nNh·∫≠p l·ª±a ch·ªçn (1-5): ").strip()
    
    if choice == "1":
        fps = int(input("T·ªëc ƒë·ªô hi·ªÉn th·ªã (fps, recommended 10-30): ") or "10")
        num_games = int(input("S·ªë games mu·ªën xem (1-10): ") or "3")
        tester.play_visual(num_games=num_games, fps=fps, greedy=True)
    
    elif choice == "2":
        num_eps = int(input("S·ªë episodes ƒë·ªÉ ƒë√°nh gi√° (recommended 100-500): ") or "100")
        tester.evaluate(num_episodes=num_eps, greedy=True)
    
    elif choice == "3":
        num_eps = int(input("S·ªë episodes m·ªói strategy (recommended 50-100): ") or "50")
        tester.compare_strategies(num_episodes=num_eps)
    
    elif choice == "4":
        num_eps = int(input("S·ªë episodes ƒë·ªÉ ph√¢n t√≠ch (recommended 20-50): ") or "20")
        tester.analyze_behavior(num_episodes=num_eps)
    
    elif choice == "5":
        print("\n Ch·∫°y t·∫•t c·∫£ tests...\n")
        print(" Test 1: Visual Play")
        tester.play_visual(num_games=3, fps=15, greedy=True)
        
        print("\n  Test 2: Performance Evaluation")
        tester.evaluate(num_episodes=100, greedy=True)
        
        print("\n  Test 3: Strategy Comparison")
        tester.compare_strategies(num_episodes=50)
        
        print("\n  Test 4: Behavior Analysis")
        tester.analyze_behavior(num_episodes=20)
        
        print("\n T·∫•t c·∫£ tests ho√†n th√†nh!")
    
    else:
        print(" L·ª±a ch·ªçn kh√¥ng h·ª£p l·ªá!")